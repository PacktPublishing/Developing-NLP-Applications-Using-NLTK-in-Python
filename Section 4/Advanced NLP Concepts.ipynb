{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import threading\n",
    "import queue\n",
    "import feedparser\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = []\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queues = [queue.Queue(), queue.Queue()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWords():\n",
    "    url = 'https://timesofindia.indiatimes.com/rssfeeds/1081479906.cms'\n",
    "    feed = feedparser.parse(url)\n",
    "    for entry in feed['entries'][:5]:\n",
    "        text = entry['title']\n",
    "        if 'ex' in text:\n",
    "            continue\n",
    "        words = nltk.word_tokenize(text)\n",
    "        data = {'uuid': uuid.uuid4(), 'input': words}\n",
    "        queues[0].put(data, True)\n",
    "        print(\">> {} : {}\".format(data['uuid'], text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPOS():\n",
    "    while True:\n",
    "        if queues[0].empty():\n",
    "            break\n",
    "        else:\n",
    "            data = queues[0].get()\n",
    "            words = data['input']\n",
    "            postags = nltk.pos_tag(words)\n",
    "            queues[0].task_done()\n",
    "            queues[1].put({'uuid': data['uuid'], 'input': postags}, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractNE():\n",
    "    while True:\n",
    "        if queues[1].empty():\n",
    "            break\n",
    "        else:\n",
    "            data = queues[1].get()\n",
    "            postags = data['input']\n",
    "            queues[1].task_done()\n",
    "            chunks = nltk.ne_chunk(postags, binary=False)\n",
    "            print(\"  << {} : \".format(data['uuid']), end = '')\n",
    "            for path in chunks:\n",
    "                try:\n",
    "                    label = path.label()\n",
    "                    print(path, end=', ')\n",
    "                except:\n",
    "                    pass\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runProgram():\n",
    "    e = threading.Thread(target=extractWords())\n",
    "    e.start()\n",
    "    threads.append(e)\n",
    "\n",
    "    p = threading.Thread(target=extractPOS())\n",
    "    p.start()\n",
    "    threads.append(p)\n",
    "\n",
    "    n = threading.Thread(target=extractNE())\n",
    "    n.start()\n",
    "    threads.append(n)\n",
    "\n",
    "    queues[0].join()\n",
    "    queues[1].join()\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 591efaf0-5a01-4dd2-8471-934b2923abc3 : 'Raazi' song 'Ae Watan' will evoke patriotism\n",
      ">> ca28a101-2c7f-4271-9f5f-44f947134bc8 : 2 States @ 4: Interesting facts\n",
      ">> 016e3c34-e58b-4a65-888a-9a3455cd0b10 : Mouni Roy in Salman Khan's 'Dabangg 3'?\n",
      "  << 591efaf0-5a01-4dd2-8471-934b2923abc3 : (GPE Watan/NNP), \n",
      "  << ca28a101-2c7f-4271-9f5f-44f947134bc8 : \n",
      "  << 016e3c34-e58b-4a65-888a-9a3455cd0b10 : (PERSON Mouni/NNP), (ORGANIZATION Roy/NNP), (GPE Salman/NNP Khan/NNP), \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    runProgram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the text similarity problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSimilarityExample:\n",
    "    def __init__(self):\n",
    "        self.statements = [\n",
    "            'ruled india',\n",
    "            'Chalukyas ruled Badami',\n",
    "            'So many kingdoms ruled India',\n",
    "            'Lalbagh is a botanical garden in India'\n",
    "        ]\n",
    "    def TF(self, sentence):\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        freq = nltk.FreqDist(words)\n",
    "        dictionary = {}\n",
    "        for key in freq.keys():\n",
    "            norm = freq[key]/float(len(words))\n",
    "            dictionary[key] = norm\n",
    "        return dictionary\n",
    "\n",
    "    def IDF(self):\n",
    "        def idf(TotalNumberOfDocuments, NumberOfDocumentsWithThisWord):\n",
    "            return 1.0 + math.log(TotalNumberOfDocuments/NumberOfDocumentsWithThisWord)\n",
    "        numDocuments = len(self.statements)\n",
    "        uniqueWords = {}\n",
    "        idfValues = {}\n",
    "        for sentence in self.statements:\n",
    "            for word in nltk.word_tokenize(sentence.lower()):\n",
    "                if word not in uniqueWords:\n",
    "                    uniqueWords[word] = 1\n",
    "                else:\n",
    "                    uniqueWords[word] += 1\n",
    "        for word in uniqueWords:\n",
    "            idfValues[word] = idf(numDocuments, uniqueWords[word])\n",
    "        return idfValues\n",
    "\n",
    "    def TF_IDF(self, query):\n",
    "        words = nltk.word_tokenize(query.lower())\n",
    "        idf = self.IDF()\n",
    "        vectors = {}\n",
    "        for sentence in self.statements:\n",
    "            tf = self.TF(sentence)\n",
    "            for word in words:\n",
    "                tfv = tf[word] if word in tf else 0.0\n",
    "                idfv = idf[word] if word in idf else 0.0\n",
    "                mul = tfv * idfv\n",
    "                if word not in vectors:\n",
    "                    vectors[word] = []\n",
    "                vectors[word].append(mul)\n",
    "        return vectors\n",
    "\n",
    "    def displayVectors(self, vectors):\n",
    "        print(self.statements)\n",
    "        for word in vectors:\n",
    "            print(\"{} -> {}\".format(word, vectors[word]))\n",
    "\n",
    "    def cosineSimilarity(self):\n",
    "        vec = TfidfVectorizer()\n",
    "        matrix = vec.fit_transform(self.statements)\n",
    "        for j in range(1, 5):\n",
    "            i = j - 1\n",
    "            print(\"\\tsimilarity of document {} with others\".format(i))\n",
    "            similarity = cosine_similarity(matrix[i:j], matrix)\n",
    "            print(similarity)\n",
    "\n",
    "    def demo(self):\n",
    "        inputQuery = self.statements[0]\n",
    "        vectors = self.TF_IDF(inputQuery)\n",
    "        self.displayVectors(vectors)\n",
    "        self.cosineSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ruled india', 'Chalukyas ruled Badami', 'So many kingdoms ruled India', 'Lalbagh is a botanical garden in India']\n",
      "ruled -> [0.6438410362258904, 0.42922735748392693, 0.2575364144903562, 0.0]\n",
      "india -> [0.6438410362258904, 0.0, 0.2575364144903562, 0.18395458177882582]\n",
      "\tsimilarity of document 0 with others\n",
      "[[ 1.          0.29088811  0.46216171  0.19409143]]\n",
      "\tsimilarity of document 1 with others\n",
      "[[ 0.29088811  1.          0.13443735  0.        ]]\n",
      "\tsimilarity of document 2 with others\n",
      "[[ 0.46216171  0.13443735  1.          0.08970163]]\n",
      "\tsimilarity of document 3 with others\n",
      "[[ 0.19409143  0.          0.08970163  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity = TextSimilarityExample()\n",
    "similarity.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolving anaphora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.corpus import names\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnaphoraExample:\n",
    "    def __init__(self):\n",
    "        males = [(name, 'male') for name in names.words('male.txt')]\n",
    "        females = [(name, 'female') for name in names.words('female.txt')]\n",
    "        combined = males + females\n",
    "        random.shuffle(combined)\n",
    "        training = [(self.feature(name), gender) for (name, gender) in combined]\n",
    "        self._classifier = nltk.NaiveBayesClassifier.train(training)\n",
    "\n",
    "    def feature(self, word):\n",
    "        return {'last(1)' : word[-1]}\n",
    "\n",
    "    def gender(self, word):\n",
    "        return self._classifier.classify(self.feature(word))\n",
    "\n",
    "    def learnAnaphora(self):\n",
    "        sentences = [\n",
    "            \"John is a man. He walks\",\n",
    "            \"John and Mary are married. They have two kids\",\n",
    "            \"In order for Ravi to be successful, he should follow John\",\n",
    "            \"John met Mary in Barista. She asked him to order a Pizza\"\n",
    "        ]\n",
    "\n",
    "        for sent in sentences:\n",
    "            chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent)), binary=False)\n",
    "            stack = []\n",
    "            print(sent)\n",
    "            items = tree2conlltags(chunks)\n",
    "            for item in items:\n",
    "                if item[1] == 'NNP' and (item[2] == 'B-PERSON' or item[2] == 'O'):\n",
    "                    stack.append((item[0], self.gender(item[0])))\n",
    "                elif item[1] == 'CC':\n",
    "                    stack.append(item[0])\n",
    "                elif item[1] == 'PRP':\n",
    "                    stack.append(item[0])\n",
    "            print(\"\\t {}\".format(stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John is a man. He walks\n",
      "\t [('John', 'male'), 'He']\n",
      "John and Mary are married. They have two kids\n",
      "\t [('John', 'male'), 'and', ('Mary', 'female'), 'They']\n",
      "In order for Ravi to be successful, he should follow John\n",
      "\t [('Ravi', 'female'), 'he', ('John', 'male')]\n",
      "John met Mary in Barista. She asked him to order a Pizza\n",
      "\t [('John', 'male'), ('Mary', 'female'), 'She', 'him']\n"
     ]
    }
   ],
   "source": [
    "anaphora = AnaphoraExample()\n",
    "anaphora.learnAnaphora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disambiguating word sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def understandWordSenseExamples():\n",
    "    words = ['wind', 'date', 'left']\n",
    "    print(\"-- examples --\")\n",
    "    for word in words:\n",
    "        syns = nltk.corpus.wordnet.synsets(word)\n",
    "        for syn in syns[:2]:\n",
    "            for example in syn.examples()[:2]:\n",
    "                print(\"{} -> {} -> {}\".format(word, syn.name(), example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def understandBuiltinWSD():\n",
    "    print(\"-- built-in wsd --\")\n",
    "    maps = [\n",
    "        ('Is it the fish net that you are using to catch fish ?', 'fish', 'n'),\n",
    "        ('Please dont point your finger at others.', 'point', 'n'),\n",
    "        ('I went to the river bank to see the sun rise', 'bank', 'n'),\n",
    "    ]\n",
    "    for m in maps:\n",
    "        print(\"Sense '{}' for '{}' -> '{}'\".format(m[0], m[1], nltk.wsd.lesk(m[0], m[1], m[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- examples --\n",
      "wind -> wind.n.01 -> trees bent under the fierce winds\n",
      "wind -> wind.n.01 -> when there is no wind, row\n",
      "wind -> wind.n.02 -> the winds of change\n",
      "date -> date.n.01 -> what is the date today?\n",
      "date -> date.n.02 -> his date never stopped talking\n",
      "left -> left.n.01 -> she stood on the left\n",
      "-- built-in wsd --\n",
      "Sense 'Is it the fish net that you are using to catch fish ?' for 'fish' -> 'Synset('pisces.n.02')'\n",
      "Sense 'Please dont point your finger at others.' for 'point' -> 'Synset('point.n.25')'\n",
      "Sense 'I went to the river bank to see the sun rise' for 'bank' -> 'Synset('savings_bank.n.02')'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    understandWordSenseExamples()\n",
    "    understandBuiltinWSD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.sentiment.sentiment_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordBasedSentiment():\n",
    "    positive_words = ['love', 'hope', 'joy']\n",
    "    text = 'Rainfall this year brings lot of hope and joy to Farmers.'.split()\n",
    "    analysis = nltk.sentiment.util.extract_unigram_feats(text, positive_words)\n",
    "    print(' -- single word sentiment --')\n",
    "    print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiWordBasedSentiment():\n",
    "    word_sets = [('heavy', 'rains'), ('flood', 'bengaluru')]\n",
    "    text = 'heavy rains cause flash flooding in bengaluru'.split()\n",
    "    analysis = nltk.sentiment.util.extract_bigram_feats(text, word_sets)\n",
    "    print(' -- multi word sentiment --')\n",
    "    print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markNegativity():\n",
    "    text = 'Rainfall last year did not bring joy to Farmers'.split()\n",
    "    negation = nltk.sentiment.util.mark_negation(text)\n",
    "    print(' -- negativity --')\n",
    "    print(negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- single word sentiment --\n",
      "{'contains(love)': False, 'contains(hope)': True, 'contains(joy)': True}\n",
      " -- multi word sentiment --\n",
      "{'contains(heavy - rains)': True, 'contains(flood - bengaluru)': False}\n",
      " -- negativity --\n",
      "['Rainfall', 'last', 'year', 'did', 'not', 'bring_NEG', 'joy_NEG', 'to_NEG', 'Farmers_NEG']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    wordBasedSentiment()\n",
    "    multiWordBasedSentiment()\n",
    "    markNegativity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring advanced sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.sentiment.util\n",
    "import nltk.sentiment.sentiment_analyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mySentimentAnalyzer():\n",
    "    def score_feedback(text):\n",
    "        positive_words = ['love', 'genuine', 'liked']\n",
    "        if '_NEG' in ' '.join(nltk.sentiment.util.mark_negation(text.split())):\n",
    "            score = -1\n",
    "        else:\n",
    "            analysis = nltk.sentiment.util.extract_unigram_feats(text.split(), positive_words)\n",
    "            if True in analysis.values():\n",
    "                score = 1\n",
    "            else:\n",
    "                score = 0\n",
    "        return score\n",
    "\n",
    "    feedback = \"\"\"I love the items in this shop, very genuine and quality is well maintained.\n",
    "    I have visited this shop and had samosa, my friends liked it very much.\n",
    "    ok average food in this shop.\n",
    "    Fridays are very busy in this shop, do not place orders during this day.\"\"\"\n",
    "    print(' -- custom scorer --')\n",
    "    for text in feedback.split(\"\\n\"):\n",
    "        print(\"score = {} for >> {}\".format(score_feedback(text), text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advancedSentimentAnalyzer():\n",
    "    sentences = [\n",
    "        ':)',\n",
    "        ':(',\n",
    "        'She is so :(',\n",
    "        'I love the way cricket is played by the champions',\n",
    "        'She neither likes coffee nor tea',\n",
    "    ]\n",
    "    senti = SentimentIntensityAnalyzer()\n",
    "    print(' -- built-in intensity analyser --')\n",
    "    for sentence in sentences:\n",
    "        print('[{}]'.format(sentence), end=' --> ')\n",
    "        kvp = senti.polarity_scores(sentence)\n",
    "        for k in kvp:\n",
    "            print('{} = {}, '.format(k, kvp[k]), end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- built-in intensity analyser --\n",
      "[:)] --> pos = 1.0, neg = 0.0, compound = 0.4588, neu = 0.0, \n",
      "[:(] --> pos = 0.0, neg = 1.0, compound = -0.4404, neu = 0.0, \n",
      "[She is so :(] --> pos = 0.0, neg = 0.555, compound = -0.5777, neu = 0.445, \n",
      "[I love the way cricket is played by the champions] --> pos = 0.625, neg = 0.0, compound = 0.875, neu = 0.375, \n",
      "[She neither likes coffee nor tea] --> pos = 0.0, neg = 0.318, compound = -0.3252, neu = 0.682, \n",
      " -- custom scorer --\n",
      "score = 1 for >> I love the items in this shop, very genuine and quality is well maintained.\n",
      "score = 1 for >>     I have visited this shop and had samosa, my friends liked it very much.\n",
      "score = 0 for >>     ok average food in this shop.\n",
      "score = -1 for >>     Fridays are very busy in this shop, do not place orders during this day.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    advancedSentimentAnalyzer()\n",
    "    mySentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a conversational assistant or chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def builtinEngines(whichOne):\n",
    "    if whichOne == 'eliza':\n",
    "        nltk.chat.eliza.demo()\n",
    "    elif whichOne == 'iesha':\n",
    "        nltk.chat.iesha.demo()\n",
    "    elif whichOne == 'rude':\n",
    "        nltk.chat.rude.demo()\n",
    "    elif whichOne == 'suntsu':\n",
    "        nltk.chat.suntsu.demo()\n",
    "    elif whichOne == 'zen':\n",
    "        nltk.chat.zen.demo()\n",
    "    else:\n",
    "        print(\"unknown built-in chat engine {}\".format(whichOne))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myEngine():\n",
    "    chatpairs = (\n",
    "        (r\"(.*?)Stock price(.*)\",\n",
    "            (\"Today stock price is 100\",\n",
    "            \"I am unable to find out the stock price.\")),\n",
    "        (r\"(.*?)not well(.*)\",\n",
    "            (\"Oh, take care. May be you should visit a doctor\",\n",
    "            \"Did you take some medicine ?\")),\n",
    "        (r\"(.*?)raining(.*)\",\n",
    "            (\"Its monsoon season, what more do you expect ?\",\n",
    "            \"Yes, its good for farmers\")),\n",
    "        (r\"How(.*?)health(.*)\",\n",
    "            (\"I am always healthy.\",\n",
    "            \"I am a program, super healthy!\")),\n",
    "        (r\".*\",\n",
    "            (\"I am good. How are you today ?\",\n",
    "            \"What brings you here ?\"))\n",
    "    )\n",
    "    def chat():\n",
    "        print(\"!\"*80)\n",
    "        print(\" >> my Engine << \")\n",
    "        print(\"Talk to the program using normal english\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"Enter 'quit' when done\")\n",
    "        chatbot = nltk.chat.util.Chat(chatpairs, nltk.chat.util.reflections)\n",
    "        chatbot.converse()\n",
    "\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== demo of eliza ===\n",
      "Therapist\n",
      "---------\n",
      "Talk to the program by typing in plain English, using normal upper-\n",
      "and lower-case letters and punctuation.  Enter \"quit\" when done.\n",
      "========================================================================\n",
      "Hello.  How are you feeling today?\n",
      ">Nice\n",
      "I see.  And what does that tell you?\n",
      ">quit\n",
      "Thank you for talking with me.\n",
      "\n",
      "=== demo of iesha ===\n",
      "Iesha the TeenBoT\n",
      "---------\n",
      "Talk to the program by typing in plain English, using normal upper-\n",
      "and lower-case letters and punctuation.  Enter \"quit\" when done.\n",
      "========================================================================\n",
      "hi!! i'm iesha! who r u??!\n",
      ">quit\n",
      "mom says i have to go eat dinner now :,( bye!!\n",
      "\n",
      "=== demo of rude ===\n",
      "Talk to the program by typing in plain English, using normal upper-\n",
      "and lower-case letters and punctuation.  Enter \"quit\" when done.\n",
      "========================================================================\n",
      "I suppose I should say hello.\n",
      ">quit\n",
      "Either become more thrilling or get lost, buddy.\n",
      "\n",
      "=== demo of suntsu ===\n",
      "Talk to the program by typing in plain English, using normal upper-\n",
      "and lower-case letters and punctuation.  Enter \"quit\" when done.\n",
      "========================================================================\n",
      "You seek enlightenment?\n",
      ">quit\n",
      "Plan well\n",
      "\n",
      "=== demo of zen ===\n",
      "***************************************************************************\n",
      "                                Zen Chatbot!                               \n",
      "***************************************************************************\n",
      "         \"Look beyond mere words and letters - look into your mind\"        \n",
      "* Talk your way to truth with Zen Chatbot.\n",
      "* Type 'quit' when you have had enough.\n",
      "***************************************************************************\n",
      "Welcome, my child.\n",
      ">quit\n",
      "When you're enlightened, every word is wisdom.\n",
      "\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      " >> my Engine << \n",
      "Talk to the program using normal english\n",
      "================================================================================\n",
      "Enter 'quit' when done\n",
      ">Hi my Engine\n",
      "What brings you here ?\n",
      ">quit\n",
      "I am good. How are you today ?\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for engine in ['eliza', 'iesha', 'rude', 'suntsu', 'zen']:\n",
    "        print(\"=== demo of {} ===\".format(engine))\n",
    "        builtinEngines(engine)\n",
    "        print()\n",
    "    myEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
